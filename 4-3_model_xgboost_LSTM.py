# âœ… ì •ê·œí™” + LSTM + XGBoost ì•™ìƒë¸” (2024ë…„ê¹Œì§€ í•™ìŠµ, 2025ë…„ ì˜ˆì¸¡)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import datetime
import matplotlib.dates as mdates

# ğŸ“Œ 1. ë°ì´í„° ë¡œë”©
merged_df = pd.read_csv(r"C:\Users\bona_\OneDrive\Desktop\FINAL PROJECT 1\ì½”ë“œ\merged_df.csv")
merged_df["ë‚ ì§œ"] = pd.to_datetime(merged_df["ë‚ ì§œ"])
merged_df = merged_df.dropna().sort_values("ë‚ ì§œ").reset_index(drop=True)

# ğŸ“Œ 2. ì •ê·œí™” (MinMax)
def parse_number(x):
    if isinstance(x, str):
        x = x.replace(",", "")
        if 'K' in x:
            return float(x.replace('K', '')) * 1e3
        elif 'M' in x:
            return float(x.replace('M', '')) * 1e6
        elif 'B' in x:
            return float(x.replace('B', '')) * 1e9
        else:
            try:
                return float(x)
            except:
                return np.nan
    return x

merged_df["ê±°ë˜ëŸ‰"] = merged_df["ê±°ë˜ëŸ‰"].apply(parse_number)

# ğŸ“Œ 3. í•™ìŠµ/ì˜ˆì¸¡ ë°ì´í„° ë¶„í• 
train_df = merged_df[merged_df["ë‚ ì§œ"] < "2025-01-01"]
future_df = merged_df[merged_df["ë‚ ì§œ"] >= "2025-01-01"]

# ì •ê·œí™”ëŠ” í•™ìŠµ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ fit
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_df[["ì¢…ê°€", "ê±°ë˜ëŸ‰", "ê°€ì¤‘ ê°ì„± ì ìˆ˜"]])
train_scaled_df = pd.DataFrame(train_scaled, columns=["ì¢…ê°€", "ê±°ë˜ëŸ‰", "ê°ì„±"])
train_scaled_df = train_scaled_df.apply(pd.to_numeric, errors="coerce").dropna().reset_index(drop=True)

# ğŸ“Œ 4. Dataset í´ë˜ìŠ¤ ì •ì˜
class ETFLSTMDataset(Dataset):
    def __init__(self, df, window_size=5):
        self.X, self.y = [], []
        data = df.values
        for i in range(window_size, len(data)):
            self.X.append(data[i-window_size:i])
            self.y.append(data[i][0])
        self.X = np.array(self.X, dtype=np.float32)
        self.y = np.array(self.y, dtype=np.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)

# ğŸ“Œ 5. Train Dataset ì¤€ë¹„
window_size = 5
dataset = ETFLSTMDataset(train_scaled_df, window_size)
train_loader = DataLoader(dataset, batch_size=32)

# ğŸ“Œ 6. LSTM ëª¨ë¸ ì •ì˜
class ETF_LSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        _, (hn, _) = self.lstm(x)
        return self.fc(hn[-1]).squeeze()

# ğŸ“Œ 7. ëª¨ë¸ í•™ìŠµ
model = ETF_LSTM(input_size=3).to("cpu")
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()
epochs = 10

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = loss_fn(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")

# ğŸ“Œ 8. LSTM + XGBoost ì˜ˆì¸¡ìš© í”¼ì²˜ êµ¬ì„±
train_lstm_preds, train_true_vals = [], []
model.eval()
with torch.no_grad():
    for x_batch, y_batch in train_loader:
        output = model(x_batch)
        train_lstm_preds.extend(output.view(-1).numpy())
        train_true_vals.extend(y_batch.view(-1).numpy())
train_residuals = np.array(train_true_vals) - np.array(train_lstm_preds)

# XGBoost í•™ìŠµìš© í”¼ì²˜ ë§Œë“¤ê¸°
def make_xgb_features(df, window_size=5):
    features = []
    for i in range(window_size, len(df)):
        window = df.iloc[i-window_size:i]
        feat = []
        for _, row in window.iterrows():
            feat.extend([row["ì¢…ê°€"], row["ê±°ë˜ëŸ‰"], row["ê°ì„±"]])
        features.append(feat)
    return np.array(features)

xgb_train_X = make_xgb_features(train_scaled_df, window_size)
xgb_model = XGBRegressor(n_estimators=100)
xgb_model.fit(xgb_train_X, train_residuals)

# âœ… í•™ìŠµì…‹ ì‹œê°í™” (ì •ê·œí™” í•´ì œ)
train_true_vals_rescaled = scaler.inverse_transform(
    np.concatenate([np.array(train_true_vals).reshape(-1, 1), np.zeros((len(train_true_vals), 2))], axis=1)
)[:, 0]
train_preds_rescaled = scaler.inverse_transform(
    np.concatenate([(np.array(train_lstm_preds) + xgb_model.predict(xgb_train_X)).reshape(-1, 1), np.zeros((len(train_true_vals), 2))], axis=1)
)[:, 0]
train_dates = train_df["ë‚ ì§œ"].iloc[window_size:]

plt.figure(figsize=(12, 5))
plt.plot(train_dates, train_true_vals_rescaled, label="ì‹¤ì œ ETF (Train)")
plt.plot(train_dates, train_preds_rescaled, label="ì˜ˆì¸¡ ETF (Train)", linestyle="--")
plt.title("ğŸ“ˆ í•™ìŠµì…‹ ì˜ˆì¸¡ ê²°ê³¼")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("ETF ì¢…ê°€")
plt.legend()
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# ğŸ“Œ 9. 2025ë…„ ì˜ˆì¸¡ ì¤€ë¹„
future_scaled = scaler.transform(future_df[["ì¢…ê°€", "ê±°ë˜ëŸ‰", "ê°€ì¤‘ ê°ì„± ì ìˆ˜"]])
future_scaled_df = pd.DataFrame(future_scaled, columns=["ì¢…ê°€", "ê±°ë˜ëŸ‰", "ê°ì„±"])

# LSTM ì˜ˆì¸¡ìš© ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°
full_df = pd.concat([train_scaled_df, future_scaled_df], ignore_index=True)
X_future = []
for i in range(len(train_scaled_df), len(full_df)):
    if i - window_size < 0:
        continue
    X_future.append(full_df.iloc[i-window_size:i].values)
X_future_tensor = torch.tensor(X_future, dtype=torch.float32)

# LSTM ì˜ˆì¸¡
model.eval()
with torch.no_grad():
    lstm_future_preds = model(X_future_tensor).numpy()

# XGBoost í”¼ì²˜ ë§Œë“¤ê¸°
xgb_future_X = make_xgb_features(full_df, window_size)[len(train_scaled_df) - window_size:]
residual_future = xgb_model.predict(xgb_future_X)
final_preds = lstm_future_preds + residual_future

# ğŸ“Œ 10. ì—­ì •ê·œí™” ë° ì‹œê°í™”
future_dates = future_df["ë‚ ì§œ"].iloc[:len(final_preds)]
final_preds_rescaled = scaler.inverse_transform(
    np.concatenate([final_preds.reshape(-1, 1), np.zeros((len(final_preds), 2))], axis=1)
)[:, 0]

# ì‹¤ì œ 2025 ETFì™€ ë¹„êµ
true_future_etf = future_df["ì¢…ê°€"].iloc[:len(final_preds)].values

# í‰ê°€ ì§€í‘œ
rmse = np.sqrt(mean_squared_error(true_future_etf, final_preds_rescaled))
mae = mean_absolute_error(true_future_etf, final_preds_rescaled)
r2 = r2_score(true_future_etf, final_preds_rescaled)
print("\nğŸ“Š [2025ë…„ ì˜ˆì¸¡ í‰ê°€ ê²°ê³¼]")
print(f"âœ… RMSE: {rmse:.2f}")
print(f"âœ… MAE:  {mae:.2f}")
print(f"âœ… RÂ²:   {r2:.4f}")

# ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(future_dates, true_future_etf, label="ì‹¤ì œ ETF (2025)", linewidth=2)
plt.plot(future_dates, final_preds_rescaled, label="ì˜ˆì¸¡ ETF (2025)", linestyle="--")
plt.gca().xaxis.set_major_locator(mdates.DateLocator(interval=10))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
plt.title("ğŸ“ˆ ETF ì§€ìˆ˜ ì˜ˆì¸¡ (2025ë…„)")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("ETF ì¢…ê°€")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()







pred_df = pd.DataFrame({
    "ë‚ ì§œ": future_dates,
    "ì‹¤ì œ ETF": true_future_etf,
    "ì˜ˆì¸¡ ETF": final_preds_rescaled
})
pred_df.to_csv("etf_predictions_2025.csv", index=False, encoding="utf-8-sig")

# ğŸ“Œ 12. ì˜ˆì¸¡ ì‹¤íŒ¨ì¼ ì¶”ì¶œ ë° ì €ì¥
pred_df["ì˜¤ì°¨"] = np.abs(pred_df["ì‹¤ì œ ETF"] - pred_df["ì˜ˆì¸¡ ETF"])
failure_df = pred_df.sort_values(by="ì˜¤ì°¨", ascending=False).head(20)
failure_df.to_csv("etf_prediction_failures.csv", index=False, encoding="utf-8-sig")


import matplotlib.font_manager as fm
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ğŸ“Œ 13. ì˜ˆì¸¡ ì‹¤íŒ¨ì¼ ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(future_dates, true_future_etf, label="ì‹¤ì œ ETF (2025)", linewidth=2)
plt.plot(future_dates, final_preds_rescaled, label="ì˜ˆì¸¡ ETF (2025)", linestyle="--")
plt.scatter(failure_df["ë‚ ì§œ"], failure_df["ì‹¤ì œ ETF"], color="red", label="ì˜ˆì¸¡ ì‹¤íŒ¨ì¼", zorder=5)
plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
plt.title("2025ë…„ ETF ì˜ˆì¸¡, ì‹¤ì œ, ì‹¤íŒ¨ì¼")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("ETF ì¢…ê°€")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ğŸ“Œ 14. ì‹¤íŒ¨ì¼ ì›ì¸ íƒìƒ‰ìš© ì¶œë ¥
print("\nğŸ“Œ ì˜ˆì¸¡ ì˜¤ì°¨ í° ìƒìœ„ 5ê°œ ì¼ì:")
print(failure_df.head(5))
print("\nì˜¤ì°¨ í‰ê· :", failure_df["ì˜¤ì°¨"].mean())

print("\nâœ… ì˜ˆì¸¡ ê²°ê³¼ ë° ìƒìœ„ ì˜¤ë¥˜ì¼ ì‹œê°í™”/ì¶œë ¥ ì™„ë£Œ!")
